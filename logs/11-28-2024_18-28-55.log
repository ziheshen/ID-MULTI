11/28/2024 18:28:55 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

11/28/2024 18:28:57 - INFO - root - freeze vision encoder
11/28/2024 18:29:08 - INFO - root - Loading pretrained model from /root/.cache/torch/hub/checkpoints/blip-diffusion
11/28/2024 18:29:08 - INFO - root - Loading pretrained BLIP2 Qformer weights.
11/28/2024 18:29:10 - INFO - root - Loading pretrained projection lyer weights.
11/28/2024 18:29:10 - INFO - root - Loading pretrained text encoder weights.
11/28/2024 18:29:10 - INFO - root - Loading pretrained vae weights.
11/28/2024 18:29:11 - INFO - root - Loading pretrained unet weights.
11/28/2024 18:29:18 - INFO - root - Using xformers.
11/28/2024 18:29:18 - INFO - root - Building datasets...
11/28/2024 18:29:29 - INFO - root - freeze vision encoder
11/28/2024 18:29:54 - INFO - root - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth
11/28/2024 18:30:01 - INFO - __main__ - 
***** Running training *****
11/28/2024 18:30:01 - INFO - __main__ -   Num examples = 6
11/28/2024 18:30:01 - INFO - __main__ -   Num batches each epoch = 2
11/28/2024 18:30:01 - INFO - __main__ -   Num Epochs = 50
11/28/2024 18:30:01 - INFO - __main__ -   Instantaneous batch size per device = 4
11/28/2024 18:30:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
11/28/2024 18:30:01 - INFO - __main__ -   Gradient Accumulation steps = 1
11/28/2024 18:30:01 - INFO - __main__ -   Total optimization steps = 100
11/28/2024 18:30:01 - INFO - __main__ -   Total parameters number = 1995356906
11/28/2024 18:30:01 - INFO - __main__ -   Training parameters number = 576349697
