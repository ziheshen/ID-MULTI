10/18/2024 08:28:23 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

10/18/2024 08:28:27 - INFO - root - freeze vision encoder
10/18/2024 08:28:32 - INFO - root - Loaded ViT-H-14 model config.
10/18/2024 08:28:37 - INFO - root - Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10/18/2024 08:28:43 - INFO - root - Loading pretrained model from /root/.cache/torch/hub/checkpoints/blip-diffusion
10/18/2024 08:28:43 - INFO - root - Loading pretrained BLIP2 Qformer weights.
10/18/2024 08:28:45 - INFO - root - Loading pretrained projection lyer weights.
10/18/2024 08:28:45 - INFO - root - Loading pretrained text encoder weights.
10/18/2024 08:28:46 - INFO - root - Loading pretrained vae weights.
10/18/2024 08:28:47 - INFO - root - Loading pretrained unet weights.
10/18/2024 08:28:54 - INFO - root - Using xformers.
10/18/2024 08:28:54 - INFO - root - Building datasets...
10/18/2024 08:29:05 - INFO - root - freeze vision encoder
10/18/2024 08:29:31 - INFO - root - load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth
10/18/2024 08:29:32 - INFO - __main__ - 
***** Running training *****
10/18/2024 08:29:32 - INFO - __main__ -   Num examples = 5
10/18/2024 08:29:32 - INFO - __main__ -   Num batches each epoch = 3
10/18/2024 08:29:32 - INFO - __main__ -   Num Epochs = 54
10/18/2024 08:29:32 - INFO - __main__ -   Instantaneous batch size per device = 2
10/18/2024 08:29:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2
10/18/2024 08:29:32 - INFO - __main__ -   Gradient Accumulation steps = 1
10/18/2024 08:29:32 - INFO - __main__ -   Total optimization steps = 160
